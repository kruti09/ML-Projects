{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Cross-encoder"
      ],
      "metadata": {
        "id": "PSbzxxFrzm2L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YN7X58668H2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "nZMAPJOc7Hdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "iTB3KdSm7MQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "datasets = load_dataset(\"PhilipMay/stsb_multi_mt\", \"en\")"
      ],
      "metadata": {
        "id": "mjtDMRH27PPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets"
      ],
      "metadata": {
        "id": "Ry6cb1oF7SNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets['train'][0]"
      ],
      "metadata": {
        "id": "yh0PX67LCU-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "id": "t15C0b7d7Yco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset"
      ],
      "metadata": {
        "id": "lV-KEI1J-iQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sentences\n",
        "train_inputs = tokenizer(\n",
        "    datasets[\"train\"][\"sentence1\"],\n",
        "    datasets[\"train\"][\"sentence2\"],\n",
        "    padding='longest',    # Add padding to ensure consistent length (optional)\n",
        "    return_tensors='pt'      # Return PyTorch tensors\n",
        ")"
      ],
      "metadata": {
        "id": "uS6zJnNs-md3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs['input_ids'][0]"
      ],
      "metadata": {
        "id": "mmnhKhzZB4WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sentences\n",
        "valid_inputs = tokenizer(\n",
        "    datasets[\"dev\"][\"sentence1\"],\n",
        "    datasets[\"dev\"][\"sentence2\"],\n",
        "    padding='longest',    # Add padding to ensure consistent length (optional)\n",
        "    return_tensors='pt'      # Return PyTorch tensors\n",
        ")"
      ],
      "metadata": {
        "id": "UWrq49RHCqvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sentences\n",
        "test_inputs = tokenizer(\n",
        "    datasets[\"test\"][\"sentence1\"],\n",
        "    datasets[\"test\"][\"sentence2\"],\n",
        "    padding='longest',    # Add padding to ensure consistent length (optional)\n",
        "    return_tensors='pt'      # Return PyTorch tensors\n",
        ")"
      ],
      "metadata": {
        "id": "JHgNo9M-C4pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_tensor_range(tensor, new_min=-0, new_max=1):\n",
        "\n",
        "    min_val = tensor.min()\n",
        "    max_val = tensor.max()\n",
        "    print(min_val, max_val)\n",
        "\n",
        "    # Scale to [0, 1]\n",
        "    tensor_scaled = (tensor - min_val) / (max_val - min_val)\n",
        "\n",
        "    # Scale to [new_min, new_max]\n",
        "    tensor_normalized = tensor_scaled * (new_max - new_min) + new_min\n",
        "\n",
        "    return tensor_normalized\n",
        "\n",
        "# Example tensors\n",
        "labels = torch.tensor(datasets['train']['similarity_score'])\n",
        "valid_labels = torch.tensor(datasets['dev']['similarity_score'])\n",
        "test_labels = torch.tensor(datasets['test']['similarity_score'])\n",
        "\n",
        "\n",
        "# Normalize tensors to the range [-1, 1]\n",
        "normalized_labels = normalize_tensor_range(labels)\n",
        "valid_normalized_labels = normalize_tensor_range(valid_labels)\n",
        "test_normalized_labels = normalize_tensor_range(test_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "rqf6WQprcIfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_labels"
      ],
      "metadata": {
        "id": "0lOxWXXzdQWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], normalized_labels)\n",
        "validation_dataset = TensorDataset(valid_inputs['input_ids'], valid_inputs['attention_mask'], valid_normalized_labels)\n",
        "test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_normalized_labels)\n"
      ],
      "metadata": {
        "id": "tIAOHwhLDAIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "h3EUyQsnG5xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataloader = DataLoader(validation_dataset, batch_size=64)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "u-panoU_-foO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch in train_dataloader:\n",
        "#     print(batch)\n",
        "#     break"
      ],
      "metadata": {
        "id": "TQnexE2wHYWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "OUmbZRxzEyP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "8toENDaGEAS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "hDjWc8QlCDlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, bert):\n",
        "    super().__init__()\n",
        "    self.bert = bert\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.linear = nn.Linear(768, 1)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    bert_output = outputs.pooler_output\n",
        "    bert_output = self.dropout(bert_output)\n",
        "    return self.linear(bert_output)"
      ],
      "metadata": {
        "id": "9pnV4SX-HccH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8o1PF8BZaZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def train(epochs, lr, model, train_dataloader, valid_dataloader, opt_func=torch.optim.Adam):\n",
        "  history = []\n",
        "  optimizer = opt_func(model.parameters(), lr)\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    for batch in tqdm(train_dataloader):\n",
        "      input_ids = batch[0].to(device)\n",
        "      attention_mask = batch[1].to(device)\n",
        "      labels = batch[2].to(device)\n",
        "\n",
        "      output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      output = torch.sigmoid(output).squeeze()\n",
        "      loss = torch.nn.MSELoss()(output, labels)\n",
        "      train_losses.append(loss)\n",
        "      loss.backward() #calculate gradients\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "    total_loss = torch.stack(train_losses).mean().item()\n",
        "    print('epoc_training_loss', total_loss)\n",
        "    # history.append(total_loss)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in tqdm(valid_dataloader):\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        output = torch.sigmoid(output).squeeze()\n",
        "        loss = torch.nn.MSELoss()(output, labels)\n",
        "        valid_losses.append(loss)\n",
        "      total_loss = torch.stack(valid_losses).mean().item()\n",
        "      print('epoc_validation_loss', total_loss)\n",
        "    # history.append(total_loss)\n"
      ],
      "metadata": {
        "id": "1BVFnagkLG-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CrossEncoder(bert)\n",
        "model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "id": "J6-XaXq7aks7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "lr = 0.00001\n",
        "history = train(epochs, lr, model, train_dataloader, valid_dataloader)"
      ],
      "metadata": {
        "id": "8jF8Lc31adzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 6\n",
        "lr = 0.00001\n",
        "history = train(epochs, lr, model, train_dataloader, valid_dataloader)"
      ],
      "metadata": {
        "id": "XQ8rsnVCe9SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2\n",
        "lr = 0.00001\n",
        "history = train(epochs, lr, model, train_dataloader, valid_dataloader)"
      ],
      "metadata": {
        "id": "-i6DFp0gbr59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dQOh03a_h3Bl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
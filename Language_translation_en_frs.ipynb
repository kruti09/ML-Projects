{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install and upgrade necessary libraries\n",
        "!pip install --upgrade transformers datasets huggingface_hub"
      ],
      "metadata": {
        "id": "V1sEipqH6Jz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iAdQqCOwAoR"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('language_translation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VCpgiCEG1K1"
      },
      "outputs": [],
      "source": [
        "# Check for CUDA availability and set the device accordingly (GPU or CPU)\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnFq-yecHS9K"
      },
      "outputs": [],
      "source": [
        "# Log in to Hugging Face Hub for model and dataset access\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwbLfSDpHVgm"
      },
      "outputs": [],
      "source": [
        "# Load the opus_books dataset for English-French translation\n",
        "from datasets import load_dataset\n",
        "\n",
        "books = load_dataset(\"opus_books\", \"en-fr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTKGw4CrIcSJ"
      },
      "outputs": [],
      "source": [
        "books"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test data by formatting English sentences\n",
        "test_data = ['###en: ' + books['train'][i]['translation']['en'] +  ' ###fr:'  for i in range(10000,10100)]"
      ],
      "metadata": {
        "id": "FnsKlUAN61j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvWnTzyPAyhL"
      },
      "outputs": [],
      "source": [
        "# Prepare training data by formatting English and French sentence pairs\n",
        "train_data = ['###en: ' + books['train'][i]['translation']['en'] +  ' ###fr: ' + books['train'][i]['translation']['fr'] for i in range(len(books['train']))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F92myZpXAx5W"
      },
      "outputs": [],
      "source": [
        "# Import train_test_split for splitting data\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uutLj7ClAzzu"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and validation sets\n",
        "train_data, valid_data = train_test_split(train_data, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko6S24u2bzAL"
      },
      "outputs": [],
      "source": [
        "# Limit the training data size\n",
        "train_data = train_data[:10000]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the validation data size\n",
        "valid_data = valid_data[:2000]"
      ],
      "metadata": {
        "id": "zXF7SRnw8w4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mnUiwBiuDCB"
      },
      "outputs": [],
      "source": [
        "# Import AutoTokenizer and AutoModelForCausalLM from transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Specify the pre-trained model name\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"  # or the actual model path\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the padding token to the end-of-sentence token\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "1aVb66Rg-pj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnK-z-qO-zoJ"
      },
      "outputs": [],
      "source": [
        "# Import necessary PyTorch and Dataset classes\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z69MHFTTvKjz"
      },
      "outputs": [],
      "source": [
        "# Define a custom dataset class for llama model training\n",
        "class translationDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.all_input_ids = []\n",
        "        self.all_labels = []\n",
        "\n",
        "        for i in data:\n",
        "          tokenized_sentence = tokenizer(i)\n",
        "          if len(tokenized_sentence['input_ids']) <= 650:\n",
        "            self.all_input_ids.append(tokenized_sentence['input_ids'] + [tokenizer.eos_token_id])\n",
        "            for j in range(len(tokenized_sentence['input_ids'])):\n",
        "                if tokenized_sentence['input_ids'][j] == 17010 and tokenized_sentence['input_ids'][j+1] == 1658  and tokenized_sentence['input_ids'][j+2] == 25:\n",
        "                  k = j+3\n",
        "                  tokenized_sentence['input_ids'] = [-100] * (k) + tokenized_sentence['input_ids'][k:]\n",
        "                  self.all_labels.append(tokenized_sentence['input_ids']+ [tokenizer.eos_token_id])\n",
        "        print(max([len(i) for i in self.all_input_ids]))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.all_input_ids[idx]), torch.tensor(self.all_labels[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgm9E6TF-ZCM"
      },
      "outputs": [],
      "source": [
        "# Create a training dataset instance\n",
        "train_dataset = translationDataset(train_data, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6_lxRScFF9-"
      },
      "outputs": [],
      "source": [
        "# Create a validation dataset instance\n",
        "valid_dataset = translationDataset(valid_data, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWePhUwVX6Of"
      },
      "outputs": [],
      "source": [
        "# Define a Data Collator class for padding tensors\n",
        "class DataCollator:\n",
        "        def __init__(self, tokenizer):\n",
        "            self.tokenizer = tokenizer\n",
        "\n",
        "        def pad_tensors(self, tensors, padding_value=0):\n",
        "\n",
        "            return torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True, padding_value=padding_value)\n",
        "\n",
        "        def __call__(self, data):\n",
        "            output_dict = {'input_ids':[f[0] for f in data],'labels': [f[1] for f in data]}\n",
        "            output_dict['input_ids'] = self.pad_tensors(output_dict['input_ids'],\n",
        "                                                        padding_value=self.tokenizer.pad_token_id)\n",
        "            output_dict['labels'] = self.pad_tensors(output_dict['labels'],\n",
        "                                                        padding_value=-100)\n",
        "\n",
        "\n",
        "            return output_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHuDWQIhYo_m"
      },
      "outputs": [],
      "source": [
        "# Create a Data Collator instance\n",
        "data_collator = DataCollator(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we4o33JWYvF7"
      },
      "outputs": [],
      "source": [
        "# Import TrainingArguments from transformers\n",
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwD8McjZa8VP"
      },
      "outputs": [],
      "source": [
        "# Define training arguments for the Trainer\n",
        "training_args = TrainingArguments(report_to = \"none\",\n",
        "                                  per_device_train_batch_size = 3,\n",
        "                                  gradient_checkpointing = True,\n",
        "                                  num_train_epochs = 1,\n",
        "                                  eval_strategy = 'epoch',\n",
        "                                  per_device_eval_batch_size= 3,\n",
        "                                  overwrite_output_dir=True,\n",
        "                                  save_steps=1000,\n",
        "                                  bf16=True,\n",
        "                                  gradient_accumulation_steps=2,\n",
        "                                  logging_steps = 1,\n",
        "                                  logging_strategy=\"steps\"\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5agGLrjZ2XG"
      },
      "outputs": [],
      "source": [
        "# Import Trainer from transformers\n",
        "from transformers import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA-LW0RqYvYr"
      },
      "outputs": [],
      "source": [
        "# Initialize the Trainer with model, training arguments, datasets, data collator, and tokenizer\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o44LF69-bN_o"
      },
      "outputs": [],
      "source": [
        "# Start training the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "trainer.save_model(\"language_translation/MyDrive/Colab notebooks/language_translation\")"
      ],
      "metadata": {
        "id": "MbOzBeoL2YRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "r6EICApSPgt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text for each item in the test data\n",
        "generated_text = []\n",
        "for i in test_data:\n",
        "  # Encode input text into token IDs\n",
        "  input_ids = tokenizer.encode(i, return_tensors='pt')  # `pt` for PyTorch\n",
        "  # Generate text using the model\n",
        "  input_ids = input_ids.to(model.device)\n",
        "  generated_ids = model.generate(\n",
        "      input_ids,                # Input tokens\n",
        "      num_return_sequences=1,   # Number of sequences to generate\n",
        "      no_repeat_ngram_size=2,   # Prevent repetition of n-grams of size 2\n",
        "      # temperature=0.01,           # Control the randomness (lower = more deterministic)\n",
        "      # top_p=0.9,                # Nucleus sampling\n",
        "      top_k=50,                 # Top-K sampling\n",
        "      do_sample=False,           # Use sampling, not greedy decoding\n",
        "      pad_token_id=tokenizer.eos_token_id  # Padding token (important for GPT-2 and similar models)\n",
        "  )\n",
        "  # Decode the generated token IDs back to text\n",
        "  generated_text.append(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "1lHCzMCFrRLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process generated text to extract actual and predicted sentences\n",
        "predictions =[]\n",
        "for i in generated_text:\n",
        "  sen ={}\n",
        "  a= i.split(' ###fr: ')\n",
        "  b= a[0].replace('###en: ', \"\")\n",
        "  predictions.append({\"actual_english_sentence\": b, \"predicted_french_sentence\": a[1]})\n"
      ],
      "metadata": {
        "id": "0LsGxoh4WXf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for using Google Generative AI\n",
        "import time\n",
        "import os\n",
        "from pprint import pprint\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "client = genai.Client(api_key=\"AIzaSyCMWJgdb1KlR0Te7L489SfPxguz5V63bIY\")\n",
        "questions = []\n",
        "count = 0\n",
        "\n",
        "# Define the generation configuration\n",
        "generation_config = types.GenerateContentConfig(\n",
        "    response_mime_type=\"application/json\"\n",
        ")\n",
        "# Use the Generative AI model to evaluate translations\n",
        "for i in predictions:\n",
        "  prompt = f\"\"\"You are a very critical judge of one of the translation task where you will be given a pair of actual english sentences and predicted french sentences in the format {{\"actual_english_sentence\": ..., \"predicted_french_sentence\": ...}},  now your job is to predict whether the predicted french sentences corresponding to english sentences are correct or not.\\nYou output should be in the format of list like this - [{{\"correct_translation\": \"Yes/No\", 'reason':'...'}}]\". Pair of text: {i}\"\"\"\n",
        "  response = client.models.generate_content(model='gemini-2.5-flash-preview-05-20', contents=prompt, config=generation_config)\n",
        "  questions.append(response.text)\n",
        "  time.sleep(5)\n"
      ],
      "metadata": {
        "id": "7QBbHyFA5V7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ast for safely evaluating strings containing Python literals\n",
        "import ast\n",
        "result = []\n",
        "for i in questions:\n",
        "  result.append(ast.literal_eval(i))\n",
        "\n",
        "# Print the result\n",
        "print(result)"
      ],
      "metadata": {
        "id": "pJb1rCX_d0oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of 'Yes' and 'No' correct_translation entries\n",
        "flattened = [item for sublist in result for item in sublist]\n",
        "# Initialize counters\n",
        "yes_count = 0\n",
        "no_count = 0\n",
        "\n",
        "# Count Yes and No\n",
        "for entry in flattened:\n",
        "    if entry['correct_translation'] == 'Yes':\n",
        "        yes_count += 1\n",
        "    elif entry['correct_translation'] == 'No':\n",
        "        no_count += 1\n",
        "\n",
        "print(yes_count, no_count)"
      ],
      "metadata": {
        "id": "-_bNPQwiAO7I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}